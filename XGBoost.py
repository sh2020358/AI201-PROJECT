# -*- coding: utf-8 -*-
"""Untitled4.ipynb

Automatically generated by Colaboratory.

Original file is located at
    https://colab.research.google.com/drive/1sQYGI7ff3zE3lLt7cYKl70kmgN1aXaw0
"""

# Loading the required packages and libraries

import numpy as np # linear algebra
import pandas as pd # data processing, CSV file I/O (e.g. pd.read_csv)
import matplotlib.pyplot as plt
import seaborn as sns

df = pd.read_csv('train.csv')
df_test = pd.read_csv('test.csv')

df.head()

feautres_na = [features for features in df.columns if df[features].isnull().sum()>1]

feautres_na

for features in feautres_na:
    print(features,np.round(df[features].isnull().mean(),4),'%')

more_than_50_percent_misssing_value_features = [features for features in feautres_na if (np.round(df[features].isnull().mean(),4)) > 0.5]

more_than_50_percent_misssing_value_features

for feature in feautres_na:
    data = df.copy()
    data[feature] = np.where(data[feature].isnull(),1,0) # replacing missing value with 1 and rest with 0
    data.groupby(feature)['SalePrice'].mean().plot.bar()
    plt.title(feature)
    plt.show()

numerical_features = [features for features in df.columns if df[features].dtypes != 'O']
numerical_features

df[numerical_features].head()

year_features = [features for features in numerical_features if "Yr" in features or "Year" in features]
year_features

df.groupby('YrSold')['SalePrice'].median().plot()

for feature in year_features:
    if feature != "YrSold":
        data = df.copy()
        data[feature] = data['YrSold'] - data[feature]
        plt.scatter(data[feature],data['SalePrice'])
        plt.title(feature)
        plt.show()

discrete_features = [feature for feature in numerical_features if len(df[feature].unique())<25 and feature not in year_features]
discrete_features

df[discrete_features].head()

# Checking the null values
df.isnull().sum()

columns =[feature for feature in df.columns if df[feature].dtypes == 'O']
len(columns)

# function to convert categorical variables to one hot encoding
def category_onehot_multcols(multcolumns):
    df_final=final_df
    i=0
    for fields in multcolumns:
        
        print(fields)
        df1=pd.get_dummies(final_df[fields],drop_first=True)
        
        final_df.drop([fields],axis=1,inplace=True)
        if i==0:
            df_final=df1.copy()
        else:
            
            df_final=pd.concat([df_final,df1],axis=1)
        i=i+1
       
        
    df_final=pd.concat([final_df,df_final],axis=1)
        
    return df_final

# making a copy of dataframe for future use
main_df=df.copy()

# concanating the test and train files to implement one hot encoding
final_df=pd.concat([df,df_test],axis=0)

final_df.shape

final_df=category_onehot_multcols(columns)

final_df.shape

# removing duplicated columns
final_df =final_df.loc[:,~final_df.columns.duplicated()]

final_df.shape

# separating the test and training data
df_Train=final_df.iloc[:1460,:]
df_Test=final_df.iloc[1460:,:]

df_Train.shape

df_Test.shape

# dropping the "SalePrice" column from test data
df_Test.drop(['SalePrice'],axis=1,inplace=True)

# preparing data for feeding into model
X_train=df_Train.drop(['SalePrice'],axis=1)
y_train=df_Train['SalePrice']

X_train.head()

X_train['GarageYrBlt'] = pd.to_numeric(X_train['GarageYrBlt'])

X_train['GarageYrBlt']

# implementing XGBoost regressor
import xgboost
classifier=xgboost.XGBRegressor()
classifier.fit(X_train,y_train)

import pickle
filename = "finalazied_model.plk"
pickle.dump(classifier,open(filename,'wb'))

df_Test['GarageYrBlt'][1]

y_predict = classifier.predict(df_Test)

y_predict

##Creating Sample Submission file
pred=pd.DataFrame(y_predict)
sub_df=pd.read_csv('sample_submission.csv')
#datasets=pd.concat([sub_df['Id'],pred],axis=1)
sub_df['SalePrice'] = pred
#datasets.columns=['Id','SalePrice']
sub_df.to_csv('sub1.csv',index=False)

sub_df

regressor=xgboost.XGBRegressor()
booster=['gbtree','gblinear']
base_score=[0.25,0.5,0.75,1]

## Hyper Parameter Optimization

n_estimators = [100, 500, 900, 1100, 1500]
max_depth = [2, 3, 5, 10, 15]

learning_rate=[0.05,0.1,0.15,0.20]
min_child_weight=[1,2,3,4]

# Defining the grid of hyperparameters to search
hyperparameter_grid = {
    'n_estimators': n_estimators,
    'max_depth':max_depth,
    'learning_rate':learning_rate,
    'min_child_weight':min_child_weight,
    'booster':booster,
    'base_score':base_score
    }

# Set up the random search with 4-fold cross validation
from sklearn.model_selection import RandomizedSearchCV

regressor = xgboost.XGBRegressor()
random_cv = RandomizedSearchCV(estimator=regressor,
            param_distributions=hyperparameter_grid,
            cv=5, n_iter=50,
            scoring = 'neg_mean_absolute_error',n_jobs = 4,
            verbose = 5, 
            return_train_score = True,
            random_state=42)

random_cv.fit(X_train,y_train)

# finding the best estimate
random_cv.best_estimator_

# substituting the best parameters
regressor=xgboost.XGBRegressor(base_score=0.25, booster='gbtree', colsample_bylevel=1,
             colsample_bynode=1, colsample_bytree=1, gamma=0, gpu_id=-1,
             importance_type='gain', interaction_constraints='',
             learning_rate=0.1, max_delta_step=0, max_depth=2,
             min_child_weight=1, missing=None, monotone_constraints='()',
             n_estimators=900, n_jobs=0, num_parallel_tree=1, random_state=0,
             reg_alpha=0, reg_lambda=1, scale_pos_weight=1, subsample=1,
             tree_method='exact', validate_parameters=1, verbosity=None)

regressor.fit(X_train,y_train)

y_predict1 = regressor.predict(df_Test)

y_predict1

##Create Sample Submission file and Submit using ANN
pred=pd.DataFrame(y_predict1)
sub_df=pd.read_csv('sample_submission.csv')
#datasets=pd.concat([sub_df['Id'],pred],axis=1)
#datasets.columns=['Id','SalePrice']
#datasets.to_csv('housepricetuned.csv',index=False)
sub_df['SalePrice'] = pred
#datasets.columns=['Id','SalePrice']
sub_df.to_csv('sub2.csv',index=False)

pred

pred.columns = ['SalePrice']

pred

df_Train.shape

df_Test.shape

test_new = pd.concat([df_Test,pred],axis = 1)
test_new

train_new = pd.concat([df_Train,test_new],axis =0)
train_new.shape

X_train=train_new.drop(['SalePrice'],axis=1)
y_train=train_new['SalePrice']

random_cv.fit(X_train,y_train)

# finding the best estimate
random_cv.best_estimator_

# substituting the best parameters
regressor=xgboost.XGBRegressor(base_score=0.25, booster='gbtree', colsample_bylevel=1,
             colsample_bynode=1, colsample_bytree=1, gamma=0, gpu_id=-1,
             importance_type='gain', interaction_constraints='',
             learning_rate=0.1, max_delta_step=0, max_depth=2,
             min_child_weight=1, missing=None, monotone_constraints='()',
             n_estimators=900, n_jobs=0, num_parallel_tree=1, random_state=0,
             reg_alpha=0, reg_lambda=1, scale_pos_weight=1, subsample=1,
             tree_method='exact', validate_parameters=1, verbosity=None)

regressor.fit(X_train,y_train)

y_predict2 = regressor.predict(df_Test)

