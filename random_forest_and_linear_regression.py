# -*- coding: utf-8 -*-
"""Untitled3.ipynb

Automatically generated by Colaboratory.

Original file is located at
    https://colab.research.google.com/drive/1z7zdkWvSDZJgNzj0qVDKjpC7E0cZyZpt
"""

import pandas as pd

df = pd.read_csv('amsterdam.csv')
df

from sklearn import preprocessing
le = preprocessing.LabelEncoder()
le.fit(df['Price'])
df['Price'] = le.transform(df['Price'])

X = df.drop(['Price'], axis = 1)
y = df['Price']
print(X)
print(y)

# this part of the code is not implemented in whole of the process or you can say that is not included
from sklearn.datasets import make_classification
from sklearn.linear_model import LogisticRegression
from sklearn.model_selection import train_test_split
from sklearn.pipeline import make_pipeline
from sklearn.preprocessing import StandardScaler

X, y = make_classification(random_state=42)
X_train, X_test, y_train, y_test = train_test_split(X, y, test_size = 0.3, random_state=4)
pipe = make_pipeline(StandardScaler(), LogisticRegression())
pipe.fit(X_train, y_train)  # apply scaling on training data


pipe.score(X_test, y_test)
# here in the above line we are spliting the data set and training and testting it 
# the major thing of that is we are taking the 70 percent of the data randomly and that is being tested randomly



# apply scaling on testing data, without leaking training data.

import numpy as np
import pandas as pd
import seaborn as sns
import matplotlib.pyplot as plt
from sklearn import preprocessing, svm
from sklearn.model_selection import train_test_split
from sklearn.linear_model import LinearRegression
regr = LinearRegression()

regr.fit(X_train, y_train)
print(regr.score(X_test, y_test))

y_pred = regr.predict(X_train)

# visulizing the difference between actual prices and the predicted priices

plt.scatter(y_train, y_pred)
plt.xlabel('Price')
plt.ylabel('Predicted Prices')
plt.title('Predicted price vs Price')
plt.show()

# linear regression checking out the normality of the errors:


sns.displot(y_train-y_pred)
plt.title('Histogram of the residuals')
plt.xlabel('Residuals')
plt.ylabel('Frequency')
plt.show()

y_test_pred = regr.predict(X_test)

from sklearn import metrics
acc_linreg = metrics.r2_score(y_test, y_test_pred)
print('R^2 :', acc_linreg)
print('Adjusted R^2:', 1-(1-metrics.r2_score(y_test,y_test_pred))* len((y_test) - 1)/ (len(y_test) - X_test.shape[1]-1))
print('MAE', metrics.mean_absolute_error(y_test, y_test_pred))
print('MSE', metrics.mean_squared_error(y_test, y_test_pred))
print('RMSE', np.sqrt(metrics.mean_squared_error(y_test, y_test_pred)))

#random forest regressor
from sklearn.ensemble import RandomForestRegressor
reg = RandomForestRegressor()
reg.fit(X_train, y_train)

y_pred = reg.predict(X_train)

from sklearn import metrics
acc_linreg = metrics.r2_score(y_train, y_pred)
print('R^2 :', acc_linreg)
print('Adjusted R^2:', 1-(1-metrics.r2_score(y_train,y_pred))* len((y_train) - 1)/ (len(y_train) - X_train.shape[1]-1))
print('MAE', metrics.mean_absolute_error(y_train, y_pred))
print('MSE', metrics.mean_squared_error(y_train, y_pred))
print('RMSE', np.sqrt(metrics.mean_squared_error(y_train, y_pred)))

#visulizing the difference between actual prices and the predicted value
y_pred = reg.predict(X_train)
plt.scatter(y_train, y_pred)
plt.xlabel('Price')
plt.ylabel('Predicted Prices')
plt.title('Predicted price vs Price')
plt.show()

# random forest checking out the normality of the errors:


sns.displot(y_train-y_train-y_pred)
plt.title('Histogram of the residuals')
plt.xlabel('Residuals')
plt.ylabel('Frequency')
plt.show()

#random forest model evaluation